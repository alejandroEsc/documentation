---
title: Migrate from Cluster and Console Custom Resources
---

<head>
    <meta name="title" content="Migrate from Cluster and Console Custom Resources| Redpanda Docs"/>
    <meta name="description" content="To ensure compatibility with future versions of Redpanda and to benefit from new features, enhancements, and security updates, you must migrate from the deprecated Cluster and Console custom resources to the Redpanda custom resource."/>
    <link rel="canonical" href="https://docs.redpanda.com/docs/migrate/kubernetes/operator/" />
</head>

To ensure compatibility with future versions of Redpanda and to benefit from new features, enhancements, and security updates, you must migrate from the deprecated Cluster and Console custom resources to the Redpanda custom resource. The migration process involves deploying at least version 23.2 of the Redpanda Operator in the same Kubernetes cluster as your existing deprecated Redpanda Operator.

For a description of what's changed, see [Deprecated Cluster and Console Custom Resources](../../../deprecated/cluster-resource).

## Prerequisites

Before migrating to the Redpanda Operator, you must have the name of your Cluster resource and the namespace in which it's deployed:

? multiple clusters ? migrate one at a time?

```bash
kubectl get cluster -A
```

Example output:

```bash-nocopy
NAMESPACE   NAME                AGE
redpanda    one-node-external   17m
```

If you also have a Console resource, you need the name of your Console resource and the namespace in which it's deployed:

```bash
kubectl get console -A
```

:::tip
Before implementing any changes in your production environment, Redpanda Data recommends testing the migration in a non-production environment.
:::

## Migrate to the Redpanda Operator and Helm

The Redpanda Operator complements the Redpanda Helm chart by extending Kubernetes with custom resource definitions (CRDs). These CRDs allow Redpanda clusters to be represented as native Kubernetes resources. This abstraction makes it simpler to manage the lifecycle of Redpanda deployments, as the Redpanda Operator takes the specifications declared in the CRDs and passes them to Helm for execution.

When using the Redpanda Operator, configurations defined in the custom resources are passed to Helm, which then deploys the Redpanda cluster according to these specifications.

1. Make sure that you have permission to install custom resource definitions (CRDs):

  ```bash
  kubectl auth can-i create CustomResourceDefinition
  ```

  You need cluster-level permissions to install the Redpanda Operator CRDs in the next steps.

1. Install the Redpanda Operator custom resource definitions (CRDs):

  ```bash
  kubectl kustomize https://github.com/redpanda-data/redpanda//src/go/k8s/config/crd | kubectl apply -f -
  ```

1. Install the Redpanda Operator *in the same namespace as your Cluster custom resource*:

  ```bash
  helm repo add redpanda https://charts.redpanda.com
  helm upgrade --install redpanda-controller redpanda/operator \
    --namespace <namespace> \
    --set image.repository=docker.redpanda.com/redpandadata/redpanda-operator \
    --set image.tag=v23.2.2 \
    --create-namespace
  ```

1. Ensure that the Deployment is successfully rolled out:

  ```bash
  kubectl --namespace <namespace> rollout status -w deployment/redpanda-controller-operator
  ```

  ```text-nocopy
  deployment "redpanda-controller" successfully rolled out
  ```

1. Stop the deprecated Redpanda Operator from reconciling the Cluster or Console custom resources:

  ```bash
  kubectl --namespace <namespace> annotate cluster <cluster-name> redpanda.vectorized.io/managed=”false”
  kubectl --namespace <namespace> annotate console <console-name> redpanda.vectorized.io/managed=”false”
  ```

1. Delete your Cluster resource's existing StatefulSet:

  ```bash
  kubectl --namespace <namespace> delete statefulset <cluster-name> --cascade=orphan
  ```

1. Update the label selectors of all Pods that were in the deleted StatefulSet:

  1. To get the Pod names:

    ```bash
    kubectl get pod -l app.kubernetes.io/instance=<cluster-name> --namespace <namespace>
    ```

  1. To update the label selectors, do the following for each Pod:

    ```bash
    kubectl --namespace <namespace> label pod <pod-name> app.kubernetes.io/component=redpanda-statefulset --overwrite
    ```

  The helm chart uses a selector that will usually not match what currently exists. To allow the rolling update of the pods, we must change these.

1. Adopt your existing Services:

  ```bash
  kubectl --namespace <namespace> annotate service <cluster-name> meta.helm.sh/release-name=<cluster-name> --overwrite
  kubectl --namespace <namespace> annotate service <cluster-name> meta.helm.sh/release-namespace=<namespace> --overwrite
  kubectl --namespace <namespace> label service <cluster-name> app.kubernetes.io/managed-by=Helm --overwrite
  kubectl --namespace <namespace> annotate service <cluster-name>-external meta.helm.sh/release-name=<cluster-name> --overwrite
  kubectl --namespace <namespace> annotate service <cluster-name>-external meta.helm.sh/release-namespace=<namespace> --overwrite
  kubectl --namespace <namespace> label service <cluster-name>-external app.kubernetes.io/managed-by=Helm --overwrite
  ```

  The names of these Services match the names of the Services that the Redpanda Helm chart will try to deploy when you deploy the Redpanda custom resource.
  These annotations and labels bring the existing Services under the management of Helm so that they do not get deleted and redeployed, saving you potential downtime.

Followed with a manual change to change the selectors:

kubectl -n <namespace> edit service <cluster-name>

and change the selector to:

  selector:
    app.kubernetes.io/instance: <cluster-name>
    app.kubernetes.io/name: redpanda

1. Adopt ServiceAccount:

kubectl -n $NAMESPACE annotate serviceaccount $CLUSTER_NAME meta.helm.sh/release-name=$CLUSTER_NAME
kubectl -n $NAMESPACE annotate serviceaccount $CLUSTER_NAME meta.helm.sh/release-namespace=$NAMESPACE
kubectl -n $NAMESPACE label serviceaccount $CLUSTER_NAME app.kubernetes.io/managed-by=Helm --overwrite

1. Delete PodDisruptionBudget:

kubectl -n $NAMESPACE delete PodDisruptionBudget $CLUSTER_NAME


1. Migrate your custom resource manifests to a Redpanda resource:

  ```bash
  migration-cli \
    --cluster <cluster-resource.yaml> \
    --console <console-resource.yaml> \
    --output=redpanda.yaml
  ```

1. Ensure that your migrated Redpanda custom resource is configured correctly. You can compare the [Cluster and Console CRD reference](../../../../reference/redpanda-operator/crd) to the [Redpanda CRD reference](../../../../reference/crd).

The migrated object will have the annotation to disallow management by the Redpanda operator. Here would be a good time to enable it by changed the annotation from “false” to “true”

redpanda.vectorized.io/managed=”true”

1. Deploy

  kubectl apply -f redpanda.yaml -n <namespace>

  The new Redpanda Operator will delete the Pods sequentially causing them to be redeployed using Helm and your Redpanda resource.


1. Wait for the Redpanda resource to successfully reach a `deployed` state:

  ```bash
  kubectl get redpanda --namespace <namespace> --watch
  ```

  Example output:

  ```nocopy
  NAME       READY   STATUS
  redpanda   True    Redpanda reconciliation succeeded
  ```

## Troubleshooting

While the deployment process can sometimes take a few minutes, a prolonged 'not ready' status may indicate an issue. See [Troubleshoot Redpanda in Kubernetes](../../../../manage/kubernetes/troubleshooting/troubleshoot) for more details.

If you cannot solve the issue or you need assistance during the migration process, [open a GitHub issue](https://github.com/redpanda-data/redpanda/issues/new/choose) in the Redpanda repository. Before opening a new issue, search the existing issues on GitHub to see if someone has already reported a similar problem or if any relevant discussions that can help you.

Helmrelease Retries Exhausted

flux suspend helmrelease <cluster-name> -n <namespace>

Make your changes to the Redpanda resource

Followed with

flux resume helmrelease <redpanda-cluster-name> -n <namespace>

## Rollback to the previous Redpanda Operator

As long as you have the cluster resource you may undo and revert your changes, but there may be downtime depending on how far you have moved into the migration process.

The main approach is to

Delete the redpanda resource, this should trigger a deletion of all resources created by the helmrelease
Set annotation of the cluster resource to be managed:
kubectl -n <namespace> annotate cluster <cluster-name> redpanda.vectorized.io/managed=”true”
kubectl -n <namespace> annotate console <console-name> redpanda.vectorized.io/managed=”true”

Your cluster should be fully reverted and running.


## Next steps

For information about the latest Redpanda Operator and the new Redpanda custom resource, see [Redpanda in Kubernetes](../../../../deploy/deployment-option/self-hosted/kubernetes/kubernetes-production-deployment).